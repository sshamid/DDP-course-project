shuttle1<-shuttle
levels(shuttle1$use)[1]<-"1"
levels(shuttle1$use)[2]<-"0"
shuttle1$use<-as.numeric(as.character(shuttle1$use))
LogiRegUseMinus<-glm((1-shuttle$use)~shuttle$wind,family="binomial")
glm(formula = 1-use ~ factor(wind), family = binomial(logit), data = shuttle1)
glm(use ~ wind, family='binomial', shuttle)
glm((1-shuttle$use)~shuttle$wind,family="binomial")
glm((1-shuttle1$use)~shuttle1$wind,family="binomial")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
head(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
install.packages("caret")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
head(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
head(adData)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
dim(training)
dim(testing)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(ggplot2)
qplot(CompressiveStrength, data = training)
head(training)
library(Hmisc)
cutCS <- cut2(training$CompressiveStrength, g=3)
table(cutCS)
plot(CompressiveStrength, data = training)
plot(training$CompressiveStrength)
cutCS <- cut2(training$CompressiveStrength, g=4)
qplot(training$CompressiveStrength, colour = cutCS)
qplot(CompressiveStrength, colour = cutCS, data = training)
plot(training$CompressiveStrength, colour = cutCS)
cutCS <- cut2(training$CompressiveStrength, g=4)
table(cutCS)
library(Hmisc)
tail(training)
qplot(training$CompressiveStrength)
qplot(training$Index,training$CompressiveStrength)
plot(training$CompressiveStrength)
?plot
?qplot
head(training$CompressiveStrength)
training$CompressiveStrength
qplot(y=training$CompressiveStrength)
qplot(y=CompressiveStrength, data = training, colour=age)
qplot(y=CompressiveStrength, data = training, colour=Age)
qplot(y=CompressiveStrength, data = training, colour=FlyAsh)
qplot(y=CompressiveStrength, data = training, colour=cutCS)
hist(training$SuperPlasticizer)
hist(as.numeric(training$SuperPlasticizer))
hist(as.numeric(training$SuperPlasticizer), breaks = 10)
?hist
barplot(as.numeric(training$SuperPlasticizer))
barplot(training$SuperPlasticizer)
barplot(as.factor(training$SuperPlasticizer))
qplot(SuperPlasticizer, data = training, geom = "density")
names(training)
qplot(Superplasticizer, data = training, geom = "density")
hist(Superplasticizer, data = training)
hist(training$Superplasticizer)
plot(training$Superplasticizer)
plot(y=training$Superplasticizer)
qplot(y=Superplasticizer, data = training)
hist(training$Superplasticizer)
hist(log10(training$Superplasticizer))
qplot(log10(Superplasticizer), data = training, geom = "density")
hist(log10(training$Superplasticizer))
mean(training$Superplasticizer)
sd(training$Superplasticizer)
head(training$Superplasticizer)
log10(head(training$Superplasticizer))
ILs <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ILs, method='pca', thresh = 0.8, outcome = training$diagnosis)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ILs <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ILs, method='pca', thresh = 0.8, outcome = training$diagnosis)
preProc
model1 <- train(ILs, testing$diagnosis, method='glm')
model2 <- preProcess(ILs, method='pca', thresh = 0.8, outcome = testing$diagnosis)
model1 <- train(ILs, testing$diagnosis, method='glm')
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ILs <- training[,grep('^IL', x = names(training) )]
model1 <- train(ILs, testing$diagnosis, method='glm')
ILstesting <- training[,grep('^IL', x = names(testing) )]
model1 <- train(ILstesting, testing$diagnosis, method='glm')
model2 <- preProcess(ILstesting, method='pca', thresh = 0.8, outcome = testing$diagnosis)
ILstesting <- testing[,grep('^IL', x = names(testing) )]
model1 <- train(ILstesting, testing$diagnosis, method='glm')
model2 <- preProcess(ILstesting, method='pca', thresh = 0.8, outcome = testing$diagnosis)
ILstesting <- testing[,grep('^IL', x = names(testing) )]
model1 <- train(ILstesting, testing$diagnosis, method='glm')
model2
confusionMatrix(testing$diagnosis, model2)
model1 <- train(diagnosis~., method='glm', data=training)
predictions <- predict(model1, newdata=testing)
confusionMatrix(predictions, testing$diagnosis)
model1 <- train(diagnosis~., method='glm', data=training)
library(Hmisc)
model1 <- train(diagnosis~., method='glm', data=training
)
install.packages("e1071")
model1 <- train(diagnosis~., method='glm', data=training)
predictions <- predict(model1, newdata=testing)
confusionMatrix(predictions, testing$diagnosis)
model2 <- train(training$diagnosis~., method='glm', preProcess = "pca", data=training, trControl=
trainControl(preProcOptions = list(thresh =0.8)))
confusionMatrix(testing$diagnosis, predict(model2, testing))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
model1 <- train(diagnosis~., method='glm', data=training)
predictions <- predict(model1, newdata=testing)
confusionMatrix(predictions, testing$diagnosis)
trainingILs < training[,grep('^IL', x = names(training) )]
trainingILs <- training[,grep('^IL', x = names(training) )]
model1 <- train(diagnosis~., method='glm', data=trainingILs)
model1 <- train(trainingILs$diagnosis, method='glm', data=trainingILs)
model1 <- train(trainingILs$diagnosis~., method='glm', data=trainingILs)
head(trainingILs)
head(training)
model1 <- train(trainingILs, method='glm')
model1 <- train(trainingILs, method='glm', outcome = training$diagnosis)
trainingILs <- training[,grep('^IL','diag', x = names(training) )]
head(trainingILs)
trainingILs <- training[,grep('^IL','^diag', x = names(training) )]
head(trainingILs)
trainingILs <- training[,grep('^IL' && '^diag', x = names(training) )]
?grep
trainingILs <- training[,grep(c('^IL', '^diag'), x = names(training) )]
head(trainingILs)
trainingILs <- training[,grep(c('IL', 'diag'), x = names(training) )]
head(trainingILs)
trainingILs <- training[,grep( 'diag', x = names(training) )]
head(trainingILs)
head(training$diagnosis)
?train
head(adData)
head(adData$diagnosis)
head(adData$predictos)
head(adData$predictors)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
names(adData)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
names(training)
names(testing)
?createdatapartition
?createDataPartition
rgamma(50, 3, .5)
dim(adData)
dim(inTrain)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
dim(inTrain)
dim(training)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)
dim(inTrain)
dim(trainingILS)
dim(trainingILs)
trainingILs <- training[,grep( 'diag', x = names(training) )]
dim(trainingILs)
training = adData[ inTrain,]
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
trainingILs <- training[,grep( 'diag', x = names(training) )]
dim(trainingILs)
dim(training)
model1 <- train(trainingILs, method='glm', outcome = trainingILs$IL_11)
trainingILs <- training[,grep( c('diag',"IL"), x = names(training) )]
trainingILs <- training[,grep( c('diag',"^IL"), x = names(training) )]
trainingILs <- training[, grep( 'diag|IL', names(training) )]
head(trainingILs)
model1 <- train(trainingILs, method='glm', outcome = trainingILs$diagnosis)
trainingILs <- training[, grep( 'diag|^IL', names(training) )]
head(trainingILs)
model1 <- train(trainingILs, method='glm', outcome = trainingILs$diagnosis)
model1 <- train(trainingILs$diagnosis~., method='glm')
model1 <- train(trainingILs$diagnosis~., method='glm', data=trainingILs)
predictions <- predict(model1, newdata=testing)
confusionMatrix(predictions, testing$diagnosis)
model2 <- train(trainingILs$diagnosis~., method='glm', preProcess = "pca", data=trainingILs, trControl=
trainControl(preProcOptions = list(thresh =0.8)))
confusionMatrix(testing$diagnosis, predict(model2, testing))
predictions <- predict(model1, newdata=testing)
confusionMatrix(testing$diagnosis, predictions)
predictorsILs <- predictors[, grep( '^IL', names(adData) )]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
df <- data.frame(diagnosis, predictorsILs)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
model1 <- train(training$diagnosis~., method='glm', data=training)
predictions <- predict(model1, newdata=testing)
confusionMatrix(testing$diagnosis, predictions)
set.seed(3433)
predictorsILs <- predictors[, grep( '^IL', names(adData) )]
df <- data.frame(diagnosis, predictorsILs)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
model1 <- train(training$diagnosis~., method='glm', data=training)
predictions <- predict(model1, newdata=testing)
confusionMatrix(testing$diagnosis, predictions)
set.seed(3433)
predictorsILs <- predictors[, grep( '^IL', colnames(adData) , value = TRUE)]
df <- data.frame(diagnosis, predictorsILs)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
model1 <- train(training$diagnosis~., method='glm', data=training)
predictions <- predict(model1, newdata=testing)
confusionMatrix(testing$diagnosis, predictions)
model2 <- train(training$diagnosis~., method='glm', preProcess = "pca", data=training, trControl=
trainControl(preProcOptions = list(thresh =0.8)))
confusionMatrix(testing$diagnosis, predict(model2, testing))
featurePlot(x = training[, c("roll_belt", "pitch_belt")], y = training$classe, plot = "pairs")
library(AppliedPredictiveModeling)
library(caret)
library(rattle)
library(rpart.plot)
library(randomForest)
library(ggplot2)
set.seed(137)
training_file <- "pml-training.csv"
if (!file.exists(training_file)){
dataUrl_training <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url=dataUrl_training, training_file, method="curl")
}
testing_file <- "pml-testing.csv"
if (!file.exists(testing_file)){
dataUrl_testing <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url=dataUrl_testing, testing_file, method="curl")
}
training_data <- read.csv(training_file, na.strings=c("NA",""), header=TRUE )
eval_data <- read.csv(testing_file, na.strings=c("NA",""), header=TRUE )
training_var <- colnames(training_data)
testing_var <- colnames(eval_data)
# Verify that the column names (excluding classe and problem_id) are identical in the training and # test set.
all.equal(training_var[1:length(training_var)-1], testing_var[1:length(testing_var)-1])
dim(training_data)
nav <- sapply(colnames(training_data), function(x) if(sum(is.na(training_data[, x])) > 0.9*nrow(training_data)){return(T)}else{return(F)})
training_data <- training_data[, !nav]
names(training_data)
training_data <- training_data[,8:ncol(training_data)]
names(training_data)
nsv <- nearZeroVar(training_data, saveMetrics = T)
training_data <- training_data[, !nsv$nzv]
nsv
set.seed(137)
inTrain <- createDataPartition(y=training_data$classe, p=0.6, list=FALSE)
training <- training_data[inTrain,]
testing <- training_data[-inTrain,]
featurePlot(x = training[, c("roll_belt", "pitch_belt")], y = training$classe, plot = "pairs")
featurePlot(x = training[, 1:5], y = training$classe, plot = "pairs")
qplot(roll_belt, pitch_belt, data = training, colour = classe)
set.seed(137)
modFit <- train(training$classe ~ ., data = training, method="rpart")
print(modFit, digits=3)
print(modFit$finalModel, digits=3)
predictions <- predict(modFit, newdata=testing)
cm <- confusionMatrix(predictions, testing$classe)
print(cm, digits=4)
set.seed(137)
modFit <- train(training$classe ~ .,  preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data = training, method="rpart")
print(modFit, digits=3)
# Run against testing with both preprocessings and 4-fold cross validations.
predictions <- predict(modFit, newdata=testing)
cm2 <- confusionMatrix(predictions, testing$classe)
print(cm2, digits=4)
set.seed(137)
modFit <- train(training$classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 4), data=_training)
print(modFit, digits=3)
set.seed(137)
modFit <- train(training$classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 4), data=training)
print(modFit, digits=3)
predictions <- predict(modFit, newdata=testing)
print(confusionMatrix(predictions, testing$classe), digits=4)
print(predict(modFit, newdata=eval_data))
set.seed(137)
# Train on training set with both preprocessing and cross validation.
modFit <- train(training$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=training)
print(modFit, digits=3)
fancyRpartPlot(modFit$finalModel)
# Run against testing set.
predictions <- predict(modFit, newdata=testing)
cm3 <- confusionMatrix(predictions, testing$classe)
print(cm3, digits=4)
# Run against 20 testing set provided by Professor Leek.
print(predict(modFit, newdata=eval_data))
x <- training[-ncol(training)]
head(x)
dim(x)
dim(training)
-ncol(training)
training[-1]
dim(training[-1])
dim(training[-53])
names(dim(training[-53]))
names(training[-53])
names(training[-1])
names(training[-52])
names(training[-53])
names(training[-54])
names(training["classe"])
names(training[1:ncol(training)-1])
library(doParallel)
library(foreach)
x <- training[1:ncol(training)-1]
y <- training$classe
rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x, y, ntree=ntree)
}
cm$overall[[1]]
cm3$overall[[1]]
cm4$overall[[1]]
predictions <- predict(rf, newdata=testing)
cm4 <- confusionMatrix(predictions, testing$classe)
print(cm4, digits=4)
rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x, y, ntree=ntree)
}
predictions <- predict(rf, newdata=testing)
cm4 <- confusionMatrix(predictions, testing$classe)
print(cm4, digits=4)
fancyRpartPlot(modFit$finalModel)
library(rattle)
fancyRpartPlot(modFit$finalModel)
modFit$finalMode
modFit$finalModel
fancyRpartPlot(modFit$finalModel)
?fancyRpartPlot
segData = data.frame(Case, predictors)
library(AppliedPredictiveModeling)
library(caret)
data(segmentationOriginal)
segData = data.frame(Case, predictors)
names(segmentationOrigina)
names(segmentationOriginal)
data(segmentationOriginal)
segData = data.frame(Case, predictors)
segData = data.frame("Case", predictors)
segData = data.frame(Class, predictors)
names(data(AlzheimerDisease))
names(AlzheimerDisease)
data(AlzheimerDisease)
names(AlzheimerDisease)
data(AlzheimerDisease)
colnames(AlzheimerDisease)
df <- data(AlzheimerDisease)
names(df)
head(df)
??AlzheimerDisease
?AlzheimerDisease
library(AppliedPredictiveModeling)
library(caret)
data(segmentationOriginal)
library(rattle)
training = subset(segmentationOriginal, Case == "Train")
testing = subset(segmentationOriginal, Case == "Test")
set.seed(125)
model = train(Class ~ ., method = 'rpart', data = training)
print(model)
fancyRpartPlot(model$finalModel)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
names(olive)
head(olive)
model = train(Area ~ ., method = 'rpart', data = olive)
fancyRpartPlot(model$finalModel)
newdata = as.data.frame(t(colMeans(olive)))
predict(model, newdata)
str(olive)
tail(olive)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
dim(SAheart)
dim(trainSA)
set.seed(13234)
model = train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = 'glm', family = 'binomial', data = trainSA)
trainPred = predict(model, trainSA)
testPred = predict(model, testSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
trainMissClass = missClass(trainSA$chd, trainPred)
testMissClass = missClass(testSA$chd, testPred)
trainMissClass
testMissClass
trainPred
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
set.seed(33833)
model = train(y ~ ., method = 'rf', data = vowel.train, prox = TRUE)  # This takes some time...
vi = varImp(model$finalModel)
vi = data.frame(var = 1:nrow(vi), imp = vi$Overall)
vi[order(vi$imp),]
vi
varImp(model$finalModel)
missClass(trainSA$chd, trainPred)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
set.seed(33833)
model = train(y ~ ., method = 'rf', data = vowel.train, prox = TRUE)  # This takes some time...
print(model)
vi = varImp(model$finalModel)
vi = data.frame(var = 1:nrow(vi), imp = vi$Overall)
vi[order(vi$imp),]
vi
varImp(model$finalModel)
vi[order(vi$imp),]
?randomforest
??randomforest
model = randomForest(y ~ ., method = 'rf', data = vowel.train, prox = TRUE)  # This takes some time...
varImp(model$finalModel)
model <- randomForest(y ~ ., method = 'rf', data = vowel.train, prox = TRUE)  # This takes some time...
varImp(model$finalModel)
model <- randomForest(y ~ ., method = 'rf', data = vowel.train, importance = TRUE)  # This takes some time...
varImp(model$finalModel)
model
set.seed(33833)
model = train(y ~ ., method = 'rf', data = vowel.train, prox = TRUE)  # This takes some time...
#model <- randomForest(y ~ ., method = 'rf', data = vowel.train, importance = TRUE)  # This takes some time...
print(model)
vi = varImp(model$finalModel, sort=True)
vi
model = train(y ~ ., method = 'rf', data = vowel.train, prox = TRUE, tuneGrid=expand.grid(mtry=1))  # This takes some time...
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
set.seed(33833)
model = train(y ~ ., method = 'rf', data = vowel.train, prox = TRUE, tuneGrid=expand.grid(mtry=1))  # This takes some time...
library(caret)
set.seed(33833)
model = train(y ~ ., method = 'rf', data = vowel.train, prox = TRUE, tuneGrid=expand.grid(mtry=1))  # This takes some time...
vi = varImp(model$finalModel, sort=True)
vi
vi[order(vi$imp),]
vi = data.frame(var = 1:nrow(vi), imp = vi$Overall)
vi[order(vi$imp),]
set.seed(33833)
model = train(y ~ ., method = 'rf', data = vowel.train, tuneGrid=expand.grid(mtry=1))  # This takes some time...
#model <- randomForest(y ~ ., method = 'rf', data = vowel.train, importance = TRUE)  # This takes some time...
print(model)
vi = varImp(model$finalModel, sort=True)
vi = data.frame(var = 1:nrow(vi), imp = vi$Overall)
vi[order(vi$imp),]
set.seed(33833)
model = train(y ~ ., method = 'rf', data = vowel.train, tuneGrid=expand.grid(mtry=2))  # This takes some time...
vi = varImp(model$finalModel)
vi = data.frame(var = 1:nrow(vi), imp = vi$Overall)
vi[order(vi$imp),]
set.seed(33833)
model = train(y ~ ., method = 'rf', data = vowel.train, tuneGrid=expand.grid(mtry=1))  # This takes some time...
vi = varImp(model$finalModel)
vi = data.frame(var = 1:nrow(vi), imp = vi$Overall)
vi[order(vi$imp),]
setwd("~/datasciencecoursera/developing-data-products")
